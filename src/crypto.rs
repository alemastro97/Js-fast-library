use wasm_bindgen::prelude::*;
use lz4_flex::{compress_prepend_size, decompress_size_prepended};
use miniz_oxide::deflate::compress_to_vec;
use miniz_oxide::inflate::decompress_to_vec;
use crc32fast::Hasher;
use sha2::{Sha256, Digest};


/// Calculates the CRC32 checksum of the provided data.
///
/// This function computes the CRC32 checksum (Cyclic Redundancy Check) of the input byte
/// slice using a hashing algorithm. The checksum is commonly used for error-checking in
/// data transmission and storage, as it helps verify the integrity of the data.
///
/// # Arguments
///
/// * `data` - A slice of bytes (`&[u8]`) containing the data to be hashed.
///
/// # Returns
///
/// * `u32` - The CRC32 checksum of the provided data. The result is a 32-bit unsigned integer
///   that represents the checksum, which can be used to verify data integrity.
///
/// # Performance Considerations
///
/// The time complexity of this function is O(n), where `n` is the size of the input `data` slice.
/// This is because the function processes each byte of the input data exactly once during the hashing
/// process. The CRC32 checksum is computed in a single pass, making it an efficient operation.
///
/// The function uses a hashing library to perform the CRC32 calculation. Depending on the implementation
/// of the `Hasher` used, it may employ various optimizations to make the checksum calculation faster.
/// For large data sets, the performance of CRC32 calculations remains linear (O(n)), but the overhead
/// is minimal due to the efficient hashing techniques used in modern libraries.
///
/// For extremely large datasets, you might want to process the data in chunks, depending on the available
/// memory and the specific constraints of your application.
///
/// # Example
/// ```rust
/// let data = b"Hello, world!";
/// let checksum = calculate_crc32(data);
/// println!("CRC32 checksum: {}", checksum);
/// ```
#[wasm_bindgen]
pub fn calculate_crc32(data: &[u8]) -> u32 {
    let mut hasher = Hasher::new();
    hasher.update(data);
    hasher.finalize()
}

/// Compresses the provided data and prepends its size.
///
/// This function compresses the input data using a compression algorithm and prepends
/// the size of the original data to the compressed result. This allows the compressed
/// data to be later decompressed and the original size to be retrieved without needing
/// external context or metadata.
///
/// The compression algorithm used in `compress_prepend_size` may vary based on the implementation.
/// Common compression algorithms like `gzip`, `deflate`, or `zlib` are typical choices.
///
/// # Arguments
///
/// * `data` - A byte slice (`&[u8]`) containing the data to be compressed.
///
/// # Returns
///
/// * `Vec<u8>` - A vector of bytes representing the compressed data, with the size of the original
///   data prepended. This allows the receiver to know how much data to expect when decompressing.
///
/// # Performance Considerations
///
/// The time complexity of the compression operation depends on the underlying compression algorithm.
/// For typical algorithms, such as `zlib` or `gzip`, the compression complexity is often O(n), where
/// `n` is the size of the input data. This is because the algorithm processes the data in a single pass
/// and performs various compression techniques (e.g., dictionary encoding, Huffman coding) to reduce the
/// size of the data. However, there is often a trade-off between compression speed and the degree of
/// compression achieved.
///
/// Compressing large datasets will be slower than smaller ones due to the increased number of operations
/// required. Depending on the algorithm, the compression may also require more memory, especially if the
/// data is compressed in memory-intensive formats.
///
/// # Example
/// ```rust
/// let data = b"Hello, world!";
/// let compressed = compress_data(data);
/// println!("Compressed data: {:?}", compressed);
/// ```
#[wasm_bindgen]
pub fn compress_data(data: &[u8]) -> Vec<u8> {
    compress_prepend_size(data)
}

/// Decompresses data that has been compressed with a prepended size.
///
/// This function takes a byte slice containing compressed data, which includes the size of the original
/// uncompressed data prepended to it. The function decompresses the data and returns the original data.
///
/// The decompression process removes the size information and extracts the actual uncompressed content.
///
/// # Arguments
///
/// * `data` - A byte slice (`&[u8]`) containing the compressed data with the size of the original data prepended.
///   The input data must be in the same format that was generated by a corresponding `compress_data` function,
///   where the size of the original data is included at the beginning of the compressed data.
///
/// # Returns
///
/// * `Vec<u8>` - A vector of bytes representing the decompressed (original) data.
///
/// # Errors
///
/// This function will panic if the decompression process fails. This can happen if the input data is not
/// in the expected format (e.g., corrupted data or invalid compression format). The panic message will be:
/// `"Decompression failed"`.
///
/// # Performance Considerations
///
/// The time complexity of the decompression function depends on the underlying decompression algorithm used.
/// Similar to compression, most decompression algorithms have an O(n) time complexity, where `n` is the size
/// of the compressed data. The function processes the data in a single pass to extract the original data.
///
/// Decompression is generally less resource-intensive than compression, but for large datasets, the time
/// and memory required will increase. The decompression speed also depends on the compression algorithm used
/// and the level of compression applied to the data.
///
/// # Example
/// ```rust
/// let compressed = compress_data(b"Hello, world!");
/// let decompressed = decompress_data(&compressed);
/// assert_eq!(decompressed, b"Hello, world!");  // The decompressed data should match the original
/// ```
#[wasm_bindgen]
pub fn decompress_data(data: &[u8]) -> Vec<u8> {
    decompress_size_prepended(data).expect("Decompression failed")
}

/// Compresses the provided data using the Deflate algorithm with a specified compression level.
///
/// This function uses the Deflate compression algorithm to compress the input data. The `deflate_compress`
/// function applies a compression level, which controls the trade-off between compression ratio and speed. 
/// The compression level can range from 1 (fastest compression, lowest ratio) to 9 (slowest compression, highest ratio).
/// 
/// The function internally uses the `compress_to_vec` function with a fixed compression level of 6, which is
/// a balanced option between speed and compression ratio.
///
/// # Arguments
///
/// * `data` - A byte slice (`&[u8]`) containing the data to be compressed using the Deflate algorithm.
///
/// # Returns
///
/// * `Vec<u8>` - A vector of bytes representing the compressed data.
///
/// # Compression Level
/// 
/// The compression level is set to 6 in this function, which provides a reasonable compromise between
/// compression speed and the degree of compression. The compression level can range from 1 to 9, where:
/// - `1`: Fastest compression, lowest compression ratio (least compressed)
/// - `9`: Slowest compression, highest compression ratio (most compressed)
/// - `6`: Default value, balanced speed and compression ratio.
///
/// # Performance Considerations
///
/// The time complexity of the `deflate_compress` function is O(n), where `n` is the size of the input data.
/// The Deflate algorithm works by identifying repetitive patterns in the data and encoding them in a compressed form,
/// resulting in smaller data sizes. The function performs this in a single pass, making it efficient for most use cases.
///
/// - **Compression Speed**: The function's speed is influenced by the chosen compression level. With a level of 6,
///   the function will provide a reasonable balance between speed and compression ratio. Using a lower level (e.g., 1)
///   will result in faster compression but with a larger compressed output. Using a higher level (e.g., 9) will result in
///   slower compression but with a better compression ratio (smaller output).
/// 
/// - **Memory Usage**: The memory usage depends on the internal buffering required by the Deflate algorithm, but it is generally
///   efficient for typical use cases. However, with higher compression levels (e.g., 9), the memory usage may increase to
///   achieve better compression ratios.
///
/// # Example
/// ```rust
/// let data = b"Hello, world!";
/// let compressed = deflate_compress(data);
/// println!("Compressed data: {:?}", compressed);
/// ```
#[wasm_bindgen]
pub fn deflate_compress(data: &[u8]) -> Vec<u8> {
    compress_to_vec(data, 6) // 6 is the compression level (1-9 range)
}

/// Decompresses data that has been compressed using the Deflate algorithm.
///
/// This function decompresses the input data that has been compressed using the Deflate algorithm.
/// It assumes that the data is in the format generated by `deflate_compress` or any other Deflate-based
/// compression scheme. If the input data is not valid or the decompression process fails, the function
/// will panic with the message `"Decompression failed"`.
///
/// # Arguments
///
/// * `data` - A byte slice (`&[u8]`) containing the compressed data. The data must have been compressed
///   using the Deflate algorithm and is expected to be in a valid format for decompression.
///
/// # Returns
///
/// * `Vec<u8>` - A vector of bytes representing the decompressed (original) data.
///
/// # Errors
///
/// This function will panic if the decompression fails. A failure can occur if the input data is corrupted,
/// improperly formatted, or if the Deflate decompression algorithm cannot process the data. The panic message
/// will be: `"Decompression failed"`.
///
/// # Performance Considerations
///
/// The time complexity of the `deflate_decompress` function is O(n), where `n` is the size of the compressed data.
/// The Deflate algorithm processes the compressed data in a single pass, decompressing it to recover the original data.
///
/// - **Decompression Speed**: Decompression is typically faster than compression. The time required depends on the
///   size and complexity of the input data, but for typical use cases, decompression should be efficient.
///
/// - **Memory Usage**: Decompression requires memory to store the decompressed data. The memory usage is proportional
///   to the size of the input data, but it is generally efficient compared to the compression process. However, very
///   large datasets may require more memory during decompression.
///
/// # Example
/// ```rust
/// let compressed = deflate_compress(b"Hello, world!");
/// let decompressed = deflate_decompress(&compressed);
/// assert_eq!(decompressed, b"Hello, world!");  // The decompressed data should match the original
/// ```
#[wasm_bindgen]
pub fn deflate_decompress(data: &[u8]) -> Vec<u8> {
    decompress_to_vec(data).expect("Decompression failed")
}

/// Computes the SHA-256 hash of the given input data.
///
/// This function takes a byte slice (`&[u8]`) as input and computes its SHA-256 hash using the
/// SHA-256 algorithm from the `sha2` crate. The result is returned as a `Vec<u8>`, which contains
/// the 256-bit hash in a byte vector form. SHA-256 produces a fixed-size output (32 bytes) regardless
/// of the input size.
///
/// SHA-256 is a cryptographic hash function that is commonly used for integrity checks, digital
/// signatures, and data verification. It produces a unique hash for a given input, and even a small
/// change in the input will result in a completely different hash.
///
/// # Arguments
///
/// * `input` - A byte slice (`&[u8]`) containing the data for which the SHA-256 hash will be computed.
///
/// # Returns
///
/// * `Vec<u8>` - A vector of 32 bytes representing the SHA-256 hash of the input data.
///
/// # Example
/// ```rust
/// let input_data = b"Hello, world!";
/// let hash = sha256_hash(input_data);
/// println!("SHA-256 hash: {:?}", hash);
/// ```
/// This will print the 256-bit hash of the string "Hello, world!".
///
/// # Performance Considerations
///
/// The time complexity of the SHA-256 hash computation is O(n), where `n` is the size of the input
/// data. This means the time it takes to compute the hash grows linearly with the size of the input.
///
/// - **Hash Computation Speed**: SHA-256 is designed to be fast on modern processors, but the time
///   required will increase with larger input data. However, for most typical use cases (small to
///   medium-sized data), SHA-256 is efficient and performs well.
#[wasm_bindgen]
pub fn sha256_hash(input: &[u8]) -> Vec<u8> {
    let mut hasher = Sha256::new();
    hasher.update(input);
    hasher.finalize().to_vec()
}